% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/explainability.R
\name{pdp2d}
\alias{pdp2d}
\title{Explanation gap visualization}
\usage{
pdp2d(
  model,
  x,
  vnames,
  type = "pdp",
  depth = 21,
  alpha = 2/3,
  right = 0.8,
  top = 0.95,
  digits = 1,
  parallel = TRUE,
  sample.frac = 1,
  pfunction = NULL,
  ...
)
}
\arguments{
\item{model}{A model with corresponding predict function that returns numeric values.}

\item{x}{Data frame.}

\item{vnames}{Character vector of the variable set for which the patial dependence function is to be computed.}

\item{type}{Character, either \code{"pdp"}, \code{"gap"} or \code{"both"} specifying the meaning of the colours
of scatter: either the value of the PD function or the differences between PD and the model's predcitions.
In case of \code{"both"} two plots are created.}

\item{depth}{Integer specifiying the number colours in the heat map.}

\item{alpha}{Numeric value for alpha blending of the points in the scatter plot.}

\item{right}{Position where to place the legend relative to the range of the x axis.}

\item{top}{Position where to place the legend relative to the range of the y axis.}

\item{digits}{Nuber of digits for rounding in the legend.}

\item{parallel}{Logical specifying whether computation should be parallel.}

\item{sample.frac}{fraction-size for sampling of x.}

\item{pfunction}{User generated predict function with arguments \code{pfunction(model, newdata)}.}

\item{...}{Further arguments to be passed to the \code{predict()} method of the \code{model}.}
}
\description{
Visualization of 2D PDP vs. unexplained residual predictions.
}
\details{
fw.xpy <- function(model, x, target, parallel = TRUE, sample.frac = 1, pfunction = NULL, ...){
  
  
  
  # Initialization and selection of first variable
  n <- 1
  cat("Step", n, "\n")
  
  sel   <- NULL
  trace <- NULL
  nms <- nms.full <- names(x)[-which(names(x) == target)]
  xpys <- rep(NA, length(nms))
  names(xpys) <- nms
  
  for(v in nms) xpys[which(names(xpys) == v)] <- xpy(model, x, v, viz = F, ...)
  
  sel   <- c(sel, which.max(xpys))
  trace <- c(trace, max(xpys, na.rm = T))
  print(xpys)
  cat("\n", nms.full[sel], max(xpys, na.rm = T), "\n\n")
  
  # forward selection variables such that explainability is maximized
  while(length(nms) > 1){
    n <- n + 1
    cat("Step", n, "\n")
    
    nms <- nms.full[-sel]
    xpys <- cbind(xpys, NA)
    for(v in nms) xpys[which(rownames(xpys) == v), ncol(xpys)] <- xpy(model, x, c(names(sel), v), viz = F, ...)
    
    sel <- c(sel, which.max(xpys[,ncol(xpys)]))
    colnames(xpys) <- c(paste("Step", 1:n))
    trace <- c(trace, max(xpys, na.rm = T))
    
    print(xpys)
    cat("\n", nms.full[sel], max(xpys[,ncol(xpys)], na.rm=T), "\n\n")
  }
  
  res <- list(selection.order = sel, explainability = trace, details = xpys)
  class(res) <- "vsexp"
  return(res)
}


#' @export
plot.vsexp <- function(x, ...){
  plot(0:length(x$explainability), c(0,x$explainability), type = "l", xaxt = "n", xlab = "", ylim = c(0, 1), ylab = "explainability")
  axis(1, at = 1:length(x$selection.order), labels = names(x$selection.order), las = 2)
}


#' @export
print.vsexp <- function(x, ...) print(cbind(x$selection.order, x$explainability))
}
\examples{
\dontrun{
library(pdp)
library(randomForest)
data(boston)
set.seed(42)
boston.rf <- randomForest(cmedv ~ ., data = boston)
pdp2d(boston.rf, boston, c("lstat", "rm"), type = "both")
}

}
\references{
\itemize{
    \item Szepannek, G. (2019): How Much Can We See? A Note on Quantifying Explainability of Machine Learning Models,
          \href{https://arxiv.org/abs/1910.13376}{\emph{arXiv:1910.13376 [stat.ML]}}.
  }
}
\author{
\email{gero.szepannek@web.de}
}
